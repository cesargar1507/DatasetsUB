{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"tgmzVhKhmJHM"},"source":["# Generación de texto con una red LSTM\n","En esta actividad vamos a utilizar una red recurrente LSTM para generar texto.\n","\n","El corpus que sirve de referencia para generar los textos consiste en Alicia en el País de las Maravillas, aunque se puede utilizar cualquier otro.\n","\n","El interés de esta actividad es utilizar las redes **LSTM para algo distinto de una clasificación**, aunque, como se puede observar, la calidad de los textos generados es inferior a la que se consigue con otros modelos más actuales, como los transformer.\n","\n","**Debes completar el código en las secciones indicadas con # COMPLETAR CODIGO**"]},{"cell_type":"markdown","metadata":{"id":"lUGDPYoH6Vmh"},"source":["## Instalación e importación de paquetes"]},{"cell_type":"code","metadata":{"id":"XkmvTyPxA9bp"},"source":["!pip install progressbar2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hg_-Lc8WBBiu"},"source":["import os\n","import urllib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import html\n","\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n","import pickle\n","import random\n","import progressbar\n","from tensorflow.keras import models, layers\n","from tensorflow.keras.utils import to_categorical\n","\n","try:\n","    from nltk.tokenize.moses import MosesDetokenizer\n","    detokenizer = MosesDetokenizer()\n","    use_moses_detokenizer = True\n","except:\n","    use_moses_detokenizer = False\n","\n","print(f\"use_moses_detokenizer={use_moses_detokenizer}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WtteF7Mu9r-"},"source":["## Definición de parámetros del corpus y su preprocesado"]},{"cell_type":"code","metadata":{"id":"uhtm7a7aCIr_"},"source":["corpus_url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n","corpus_path = \"alicia.txt\"\n","preprocessed_corpus_path = \"alicia_preprocessed.p\"\n","most_common_words_number = 10000\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kH8wS9yCvRXc"},"source":["## Tranformación entre palabras e índices\n","Se definen dos funciones:\n","* **encode_sequence**: codifica una secuencia de palabras en una secuencia de índices\n","* **decode_indices**: decodifica una secuencia de índices en la secuencia de palabras correspondiente"]},{"cell_type":"code","metadata":{"id":"6uqjM9GSKNxM"},"source":["def encode_sequence(sequence, vocabulary):\n","\n","    return [vocabulary.index(element) for element in sequence if element in vocabulary]\n","\n","\n","def decode_indices(indices, vocabulary):\n","\n","    decoded_tokens = [vocabulary[index] for index in indices]\n","    if use_moses_detokenizer  == True:\n","        return detokenizer.detokenize(decoded_tokens, return_str=True)\n","    else:\n","        return \" \".join(decoded_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0YFmoL9hXHi"},"source":["## Descarga del corpus\n","Solo se descarga si no se encuentra en local (Colab o disco)"]},{"cell_type":"code","metadata":{"id":"U1NGiz_3hUru"},"source":["import requests\n","\n","def download_corpus_if_necessary():\n","\n","    if not os.path.exists(corpus_path):\n","        print(\"Descargamos el corpus...\")\n","\n","        # Descargamos el contenido\n","        corpus_string = requests.get(corpus_url).content.decode('utf-8')\n","\n","        # Eliminamos etiquetas HTML\n","        corpus_string = corpus_string.replace(\"<pre>\", \"\")\n","        corpus_string = corpus_string.replace(\"</pre>\", \"\")\n","\n","        # Grabamos en fichero\n","        corpus_file = open(corpus_path, \"w\")\n","        corpus_file.write(corpus_string)\n","        corpus_file.close()\n","\n","        print(\"El corpus se ha grabado en\", corpus_path)\n","    else:\n","        print(\"El corpus ya estaba grabado\")\n","\n","download_corpus_if_necessary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yd8FM41ODLCf"},"source":["## Preprocesado del corpus\n","Solo se preprocesa si no se ha hecho antes."]},{"cell_type":"code","metadata":{"id":"ekKisSsb_fje"},"source":["def preprocess_corpus_if_necessary():\n","\n","    if not os.path.exists(preprocessed_corpus_path):\n","        print(\"Preprocesamos el corpus...\")\n","\n","        corpus_file = open(corpus_path, \"r\")\n","        corpus_string = corpus_file.read()\n","\n","        # Generación del vocabulario\n","        print(\"Extraemos las palabras (tokens)...\")\n","        corpus_tokens = word_tokenize(corpus_string)\n","        print(\"Número de tokens:\", len(corpus_tokens))\n","        print(\"Generamos el vocabulario...\")\n","        word_counter = Counter()\n","        word_counter.update(corpus_tokens)\n","        print(\"Longitud del vocabulario antes del corte:\", len(word_counter))\n","        vocabulary = [key for key, value in word_counter.most_common(most_common_words_number)]\n","        print(\"Longitud del vocabulario después del corte:\", len(vocabulary))\n","\n","        # Conversión a índices\n","        print(\"Codificamos a índices...\")\n","        indices = encode_sequence(corpus_tokens, vocabulary)\n","        print(\"Número de índices:\", len(indices))\n","\n","        # Saving.\n","        print(\"Grabamos fichero de preprocesado del corpus...\")\n","        pickle.dump((indices, vocabulary), open(preprocessed_corpus_path, \"wb\"))\n","    else:\n","        print(\"El corpus ya estaba preprocesado\")\n","\n","preprocess_corpus_if_necessary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uUGvyFcb0u4s"},"source":["## Generación del dataset\n","Enfocamos la generación del dataset como un **aprendizaje auto-supervisado (self-supervised learning)**:\n","* para generar secuencias de entrada, seleccionamos de forma aleatoria una secuencia de índices (correspondientes a palabras) del corpus\n","* para asignar una etiqueta, escogemos el índice de la palabra siguiente a la secuencia de entrada"]},{"cell_type":"code","metadata":{"id":"z79RLSh50veq"},"source":["# Parámetros de generación del dataset\n","dataset_size = 50000\n","sequence_length = 30 # longitud de los textos del dataset de entrenamiento\n","\n","def get_dataset(indices):\n","\n","    data_input = []\n","    data_output = []\n","    current_size = 0\n","    bar = progressbar.ProgressBar(max_value=dataset_size)\n","    while current_size < dataset_size:\n","\n","        # seleccionamos de forma aleatoria una secuencia de índices (correspondientes a palabras)\n","        random_index = random.randint(0, len(indices) - (sequence_length + 1))\n","        input_sequence = indices[random_index:random_index + sequence_length]\n","        # la etiqueta de la secuencia aleatoria es el índice de la siguiente palabra\n","        output_label = indices[random_index + sequence_length]\n","\n","        data_input.append(input_sequence)\n","        data_output.append(output_label)\n","\n","        current_size += 1\n","        bar.update(current_size)\n","    bar.finish()\n","\n","    # Transformamos las listas a numpy arrays\n","    data_input = np.array(data_input)\n","    data_output = np.array(data_output)\n","    return (data_input, data_output)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHOclEylxLOR"},"source":["## Definición de la red LSTM y entrenamiento\n","Solo se define y entrena si no se ha hecho antes, o si se fuerza con el parámetro train_anyway.\n","\n","Por último, se graba el modelo."]},{"cell_type":"code","metadata":{"id":"mDbtuIYeA8Rk"},"source":["# Parámetros del modelo y entrenamiento\n","train_anyway = True # fuerza el entrenamiento, aunque se haya realizado antes\n","epochs = 10 # número de interaciones a entrenar\n","batch_size = 128 # tamaño de lote\n","hidden_size = 1000 # número de nodos en la red LSTM\n","model_path = \"alicia_model.h5\"\n","\n","def train_neural_network():\n","\n","    if not os.path.exists(model_path) or train_anyway == True:\n","\n","        # Carga de las palabras del corpus y sus índices\n","        indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n","\n","        # Generación del dataset\n","        print(\"\\nGeneramos el dataset...\")\n","        data_input, data_output = get_dataset(indices)\n","        data_output = to_categorical(data_output, num_classes=len(vocabulary))\n","\n","        # Creación del modelo\n","        print(\"Creamos el modelo...\")\n","        # COMPLETAR CODIGO (debes seguir las indicaciones de la actividad para generar la red LSTM) \n"," \n","        model.summary()\n","\n","        # Definimos el resto del modelo: optimizador, función de pérdidas y métrica\n","        # COMPLETAR CODIGO\n","\n","        # Entrenamiento del modelo\n","        print(\"Entrenamos el modelo...\")\n","        # COMPLETAR CODIGO\n","        \n","        # Grabación del modelo\n","        print(\"...y grabamos el modelo\")\n","        model.save(model_path)\n","\n","        plot_history(history)\n","        \n","\n","def plot_history(history):\n","\n","    print(history.history.keys())\n","\n","    # Gráfica de función de pérdida\n","    plt.plot(history.history['loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.savefig(\"history_loss.png\")\n","    # plt.clf()\n","    plt.show()\n","    plt.close()\n","    \n","    # Gráfica de accuracy.\n","    plt.plot(history.history['categorical_accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.savefig(\"history_accuracy.png\")\n","    # plt.clf()\n","\n","    plt.show()\n","    plt.close()\n","        \n","train_neural_network()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DZP180c-g3pZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCzZpOgI3U-T"},"source":["## Generación de texto\n","\n","La generación de texto se basa en la función **get_index_from_prediction**, que obtiene un índice a partir de una predicción y un parámetro de temperatura "]},{"cell_type":"code","metadata":{"id":"mry9VsvlW2yU"},"source":["# Parámetros de generación de texto\n","generated_sequence_length = 50 # longitud de los textos generados\n","n_generated_texts = 15 # número de textos a generar\n","\n","def generate_texts(n_generated_texts=10):\n","\n","    print(f\"Generamos {n_generated_texts} textos...\")\n","\n","    # carga del corpus\n","    indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n","\n","    # carga del modelo previamente grabado\n","    model = models.load_model(model_path)\n","\n","    # Generamos tantos textos como indique el parámetro n_generated_texts\n","    for _ in range(n_generated_texts):\n","\n","        # Escogemos un parámetro aleatorio \"temperature\" para la predicción\n","        temperature = random.uniform(0.0, 1.0)\n","        print(\"Temperatura:\", temperature)\n","\n","        # Obtenemos una muestra aleatoria a modo de secuencia semilla a partir de la cual se generarán textos\n","        random_index = random.randint(0, len(indices) - (generated_sequence_length))\n","        input_sequence = indices[random_index:random_index + sequence_length]\n","\n","        # Generamos la secuenca de salida repitiendo la predicción\n","        generated_sequence = []\n","        while len(generated_sequence) < generated_sequence_length:\n","            prediction = model.predict(np.expand_dims(input_sequence, axis=0))\n","            predicted_index = get_index_from_prediction(prediction[0], temperature)\n","            generated_sequence.append(predicted_index)\n","            input_sequence = input_sequence[1:]\n","            input_sequence.append(predicted_index)\n","\n","        # Convertimos la secuencia de índices generada en una frase\n","        text = decode_indices(generated_sequence, vocabulary)\n","        print(text)\n","        print(\"\")\n","\n","        \n","def get_index_from_prediction(prediction, temperature=0.0):\n","\n","    # Temperatura cero - usamos argmax.\n","    if temperature == 0.0:\n","        return np.argmax(prediction)\n","\n","    # Temperatura distinta de cero - aplicamos cierta aleatoriedad\n","    else:\n","        prediction = np.asarray(prediction).astype('float64')\n","        prediction = np.log(prediction) / temperature\n","        exp_prediction= np.exp(prediction)\n","        prediction = exp_prediction / np.sum(exp_prediction)\n","        probabilities = np.random.multinomial(1, prediction, 1)\n","        return np.argmax(probabilities)\n","\n","\n","generate_texts(n_generated_texts)"],"execution_count":null,"outputs":[]}]}